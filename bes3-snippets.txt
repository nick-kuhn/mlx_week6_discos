TOPIC: ai_history
TITLE: The evolution of PPO from games to language models
CONTENT: Why do you think this is so complicated? Because you're trying to make something that will work generically across lots of different types of games and different types of systems. You're trying to say you're automatically playing Pac-Man or other games, and this algorithm would work for that. This was developed in 2017 to do reinforcement learning. We had other reinforcement learning algorithms already, but the PPO which we use for language models looks super similar to the PPO that people use for games. Because that's all people knew about, they kind of took this reinforcement learning specialized algorithm and applied it almost one-to-one to language models. The idea is that when something gets discovered that works very well in one domain, you try to fit it into a new domain. PPO came in 2017 and they tried to put it into language models in 2018-2019. Language models weren't really that good until maybe GPT-3 in 2020, but when GPT-3 came out, they were like "wow this is working."

TOPIC: ai_research
TITLE: When you have a hammer everything looks like a nail
CONTENT: The team with PPO was OpenAI, and OpenAI was doing reinforcement learning back then. So really it was their specialty. And when you have a hammer, every problem looks like a nail. That's what they did - they had this reinforcement learning tool and they just had to apply it to language models. I'm not criticizing people - there are good reasons for this approach. But the idea is that this was the first successful attempt to apply RL to language models. OpenAI had the expertise in reinforcement learning, they had language models that were starting to work well, and they combined the two. It's actually quite natural that they would take their existing RL algorithms and see if they could make them work for language model training. The surprising thing is how well it actually worked.

TOPIC: reinforcement_learning
TITLE: Generalized Advantage Estimation explained step by step
CONTENT: This is called Generalized Advantage Estimation (GAE). The first thing that we do is we start from the end token and work backwards recursively. We have to spread the reward from the end all the way back to each token. What I'm trying to do is calculate the advantage for each token. We want to discount future rewards because future rewards are worth less than immediate rewards. What we do is sum up rewards for the next few steps and then try to predict all the future rewards. The reason we do this is because the advantage calculation could be high variance. The generalized advantage estimation is trying to say that this reward should depend on the future, but the future could be high variance, so we discount it. We're trying to predict how good we're going to do in the future, and how many steps in the future we can look. We're going to take the actual rewards for two or three steps, and then we're going to predict what we'll get from there on.

TOPIC: ai_training
TITLE: The complex role of the value function in advantage calculation
CONTENT: The value function has to do two things, and it is kind of a big deal. We can collect the actual rewards that we observe, and then we can predict future rewards. Even if we start from one place and collect all the rewards in the future, those rewards can still be very high variance. So we actually want to predict even that. We try to predict how good we're going to do now and also how well we're going to do in the future. The value function has to estimate both current performance and future expectations. Once we have the advantage, which is how good I'm doing right now compared to how I think I was doing, we can start to backpropagate. The advantage is what we think we're doing at the moment compared to how we're actually doing. But how we're actually doing depends on rewards we haven't seen yet, which we have to estimate using the value function.

TOPIC: ai_training
TITLE: The final advantage calculation and backpropagation
CONTENT: Now we have the final number that we're looking for - the advantage. All this complexity is because we don't know how to distribute the reward fairly, and we have to be very careful in how we distribute it because rewards can be very high variance. By themselves, these high variance rewards would cause instability in training. Now we have this advantage which is exactly what we want, and we can start to backpropagate. The GAE is responsible for calculating the advantage. The advantage calculation involves the immediate reward plus what we think the next state will be worth according to the value function, minus what we think the current state is worth. When you say value function, you mean the value model - that's the model that tries to estimate how good any given state is. This value model has quite a bit of work to do, but it basically tries to estimate how well you're doing now and what you expect to do in the future.

TOPIC: ai_training
TITLE: Training the value model to reduce reward variance
CONTENT: We're going to train this value model so that the difference between the actual rewards and what we estimated the rewards to be is going to be low. The initial value model starts from the same checkpoint as the policy. We train it with a mean squared error loss. There's a specific reason we do this before we put everything together - it's because we want to reuse some of the question-answer pairs. We're going to backpropagate multiple times, three, four, five, six times. We're going to take multiple steps with the optimizer. Usually you have a question, you backpropagate once, and you forget about it and move on. But here we're going to reuse the same examples multiple times. You might say "wait a second, you just told me we don't want to do that because of catastrophic forgetting." But we have a solution for that problem, which is the policy clipping mechanism.

TOPIC: reinforcement_learning
TITLE: The on-policy versus off-policy problem in PPO
CONTENT: Let's move on to the loss function of the policy. We calculated our advantage at each time step. We do something here which is really important - we say that we are running "off-policy" because the first policy generates the answer, and the first time we backpropagate on that answer, we say it's "on-policy." The policy that is getting trained is the same one that produced the answer, which is correct. But now when we backpropagate the second time, that policy is changing, so it's kind of a new policy. When we backpropagate again, the policy that would have never produced this answer is now being trained on it. Why do we keep backpropagating on this answer that doesn't belong to the current policy? Because meanwhile the policy changed after the first backpropagation. So the first time we do it, we are on-policy. The other times, we are off-policy because the answer doesn't really belong to the current policy version.

TOPIC: ai_training
TITLE: PPO's conservative approach to policy updates
CONTENT: This is what PPO is really good at - preventing the policy from going too crazy when we train off-policy. We have a regularized system where we say that the current policy doesn't have to move too far from the old policy. Policy comes into the training step, produces an answer, and then we start to backpropagate multiple times. This policy is changing, but I don't want it to change too much because then we get catastrophic forgetting. We still want this policy that is getting backpropagated to be similar to the one at the beginning of the step. We call the original policy the "old policy" and the one that's getting updated the "new policy." Look how conservative we are: we say take the new change you calculated, and if the change is too big, we just clip it. We clip it to 95% - we allow only 5% movement. We make a mixture between the clipped and unclipped versions, and we take the minimum. This is how conservative we are about policy updates.

TOPIC: ai_training
TITLE: The three components of PPO loss function
CONTENT: Once we calculate the policy loss, we take the negative of this and call it our policy loss. Then the value loss is just mean squared error. We also have an entropy term where we try to regularize and get the model to explore more, because look how much constraint we are putting on the model's movement. We have another regularizing term - we want the entropy to be higher, and we want the model to give diverse answers. We're trying to make the model say more varied things instead of always giving the same response. This is the full loss function - policy loss, value loss, and entropy loss. As usual, we put coefficients in front of them because we want to weight these components as we wish. The policy loss uses the clipped advantage, the value loss reduces variance, and the entropy loss encourages exploration. This combination makes the training stable while still allowing the model to learn and improve.

TOPIC: ai_philosophy
TITLE: What reinforcement learning actually does for language models
CONTENT: What are we really doing with all this reinforcement learning? As the original paper suggests, we're sort of just doing sample efficiency improvement. It's not that we are putting new capabilities inside the model. Remember, the model has all the capabilities that came from pre-training. There's nothing fundamentally new that you can teach it through RL. All this reinforcement learning is just for preference alignment. If I give you a question, you should give me an answer that I prefer. The model already learned how to answer questions during pre-training. I just want it to be efficient about giving me the answer I want, rather than me having to ask 20 times before I get a good response. So the base model already has the knowledge and capabilities - we're just teaching it to surface the right answers more reliably and efficiently.

TOPIC: ai_training
TITLE: The complexity of hyperparameter tuning in PPO
CONTENT: You see that PPO has a lot of hyperparameters. They will ask you what the discounting factor you want is, what lambda you want, what epsilon you want for clipping, how many examples you want to replay, what's your learning rate, and so on. I'm just quoting from the documentation. Now you know why you have so many options to tune. Basically, I'm just taking the documentation and showing you all the hyperparameters you need to set. This is why PPO can be tricky to get right - there are many knobs to turn, and getting the right combination of hyperparameters is often more art than science. The algorithm is sophisticated, but that sophistication comes with complexity. Each hyperparameter affects the training dynamics in subtle ways, and finding the right balance often requires extensive experimentation and domain expertise.
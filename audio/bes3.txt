 I like the ten things that people think are overly confident in the dead because you are not much concerned. And then I'm not sure how much I can think of it. I can think that we ask you actually, why do you think this is so complicated? Because of the mass savings. There are a lot of movie pieces like this. This is generally understood. There's a bit more complex of the model. Let's try and say that we can get confused together. Because you're trying to solve this thing. You're trying to make something that will work generically across lots of different types of games, both of which are all different types of systems to try and say you're automatically playing Pac-Man against this type of open and open. I guess this algorithm would work for that whereas not easily thrown in our limit that way. I would say that kind of comment there. I will give you a little bit more from my horizon, which is... This was developed in 2017 to try to do... We had other reinforcement like big mind already was doing many things. And stuff with other type of form weight and stuff. And this looks... The PPO which we use while in Atlanta looks super similar to the PPO that people do like a lot of things. And because that's all people knew about that. They kind of took this reinforcement learning specialized algorithm. And they almost took it one on one. They actually never looked at it one on one and applied to that algorithm. And clearly changing the main a little bit. And people sort of different study and they sort of just pull this apart. And they actually look at the way most people suffer. But the idea is that I think again the first thing that something gets discovered, that you can't go from someone else which is very impressive or very messy. For both world for that domain you try to fit it in this new domain. This was the first one. I think PPO came in 2017 and they tried to put it to Atlanta in 2018-2019 which is the 21st. And Atlanta was not really that good. Up until maybe GPT. And the GPT was in 2020. I think 2019. And I mean future GPT too. The only thing that is surprising about this is actually talks as a nice distribution of token. And then every something makes accurate. But forget the Atlanta was able to do anything. And those sub-tropies were in GPT 3, the first version of the PQP2, where there was a scale of it and they were like, wow this is working. But we finished the money for labor as a potential next thing back to you now. And the guy with the PPO was open out. And open out was in the office back then. So really it was my learning. And I like this is some of Cisco. So that's what they had. And when you have a hunger, every problem looks like a nail. And that's what you do. Like this is a damn thing. I just have to smash this thing. No, don't be very poor. And I'm not like anything people like also, I mean there's three as buffers. I can go to component in our labor. I don't know how much of both are missing. I think it is a buff. What am I doing here? I think you know what? For other people, co-in that. Let me try. I think this is a good idea of my turn. Can you bring me to a job to be here and ask me to write a new mark? I wouldn't have asked you that really big. I mean this is something that we really couldn't draw. But I think we don't expect that. So this is called generalized in the unconscious dimension. And I think we have a little bit of a joy that we can easily go through. Ah. I was asking you to make an example of the secret that we want to put in. Okay. This is the more classic. The value. The idea here. I think I also thought that it would be quite interesting to make a challenge in fun. Can you embrace this joy? No, I think I was going to try to keep it going in general terms. So I don't want to have the joy in it. And I think that's going to be much better. You kind of have to believe me here. The first thing that we do is we start from the enemy token. So the end token here is simply a big T. Yeah. So this one is just a. So. A. This is right. This is going to be. This is going to be a sensor. I don't think you do that. Which case may be for a while. That's a little bit faster. Don't you want to solve this equation? These are by sort of a cursively start on the end of it. And then kind of remove back. So what's the answer? Okay. I'm going to say this in way. I can show you. Is that. We have to spread that the reward that. From here all of the ways to say make sure token up. And then this is useful because what I'm trying to do is that. From each of those token reward that. Remember that what we want to do is the token reward itself. The discount of the care that I'm using. But then to us. We also want to know what the expectation that you. How many points you're going to collect that. For the token rewards. We kind of want to discount that. We want to discount it because like. Future rewards is worth less than. The general. But we do. And here will be. We do this. We do that. What we do is. We think of a lot of one of this one. We sort of sum up. Two of three. We sum up the next one. And then we kind of try to predict all the future rewards. And the reason we do this is again because of our answer. And so it's going to be ideal. But this generalized advance that estimation. It's not trying to say that like. This or more should depend on the future. Or the future. It could be like high by. So sort of is not a little. The future was discounted. That the. Of the. What do you think I'm doing right now. So you kind of have the body of the. Use that in a couple of ways. And this is where we kind of see this form. Yeah. We're sort of trying to predict. How good we're going to do future. Which is this talent. How many steps in the future. We can kind of look. And we're going to scoundrel the rest of the. So we're going to sort of take the action of work for two or three steps. You are kind of going to put the arm. Match. We're going to get from there on. Now. Minus. What do we think we're doing. I know this is a bit complex. And I was showing by this is very important. Because allows us to sort of reduce by and even further. So we just have to believe in that. And this one was a within our topic. We have a key class. And the C plus. Expand. We see no further. We can kind of like collect the actual rewards that we did. And then we can just predict for their rewards. Because even if we sort of start from one place and we collect all the rewards in the future. We kind of discard. Those were all can still be very big. So actually want to predict even that. So try to predict how good they're going to do. And how well we are right now. And also how well we're going to do the future. So the value function has to do two things. And it is kind of a big deal. Because I really like to take hints of bias. Be it a share of my answer. Dab it. I know that you know super clear. But I can make it more clear. And you also need a little bit of thinking around it. And I want to be like well in this greatest. Because looks a little bit complex. Once we have this R.P. Which is the advantage which itself becomes the body model. Class is sort of a value here that shows up again. Which is like how good I'm doing right now. Compared to how I think I was doing class. So yeah, this is digital. And I like how what do you think I'm doing at the moment. Class, how I'm actually doing it. But how I'm actually doing itself. It's something that depends a little more than the end. Which we have to discover. And the discovery itself uses the value function. Once we have this R.T. So now we have the final. The final number that we're looking for. And the real craziness and all these craziness is because we don't know how to square the final order. And we have to be very careful in how it's square because this work can be very clear. And by itself to sort of pull us out of this confusion in some ways. Now we have this R.T. which is the exact one we want. And we can start to backpropagate that. So as the G.A. is the square of the R.T. That's exactly what the G.A. is doing. And so if we go back here. That's where it happened. The equations are now. Yeah. And because the equations are the same one that's here. So we have a like a whole different level. Which is this sort of delta P. Which this should get further to be more. Because usually we do have a C plus R.T. Plus one sort of like the word in the next step. It results in actualizing to the reward now. Plus what we think that it wants to be from there on with the value function. When you say value function means value model. Value model. Yeah, that's an only certificate. So this value model is quite a bit of work to do. But basically just try to estimate everything of how you do it and how you want to do the future. How you do it now and what you think your expectation will be. You know what I'm trading for you really? Here with the rules we have to add up. We know we know eventually how it did that. And we know what the model did. So we're trying to convince but now we're going to trade this value model to sort of change the actual rewards. So that is where the device between the reward is up and what we estimated the reward to be is going to be low. Again, just provide in the machine. So here's to you. The initial value model is also. Now to the same. Yeah, the value chain back there. That's also a little requested. This is a quick thing. The last thing I guess the same. Yeah, it's same. And we trade it with the means for that. So this is one part of the function. Where this experiment butter here. I see we just go out of any way. There's a single reason for this before we put the liquid in here. It's because we want to reuse some of the question answers. So we kind of propagate multiple times. Usually you have like question ones you back propagate once and you forget about it and you move on with your life. Here what I'm going to tell you is that we're going to propagate three four five six times. We're going to multiple steps in the optimizer. We show you say wait a second. That was one. You just told me that we don't want to do that. If we use this and we propagate once, it's a really forget this stuff. If we want to get five times, we the same example. It's game over. But remember, that's a problem. But we have solution. We ever. So now we can go to the. Let's move on to this side of the equation which is the lot of function of the proxy. We calculated our advantage at each time. Are you away? Our T here books are full in. So I actually do my covered that. Well, we do an append here which is really because bottom barrier. we say that we are running to off policy because the first policy to make the answer and the first time that we make up again that answer we say it's all policy. So the policy that is getting trained is the same one that actually produced the answer which is correct is what you expect then. But now when we both propagate the first time that policy is changing stuff is a kind of new person as well as the last time. But what I told you step on the goal is that actually we propagate again. I said wait a second there would have never produced the answer now. Why you kind of keep back propagate on this answer that is not mine because I changed after this is first back propagation. So now you are kind of training me on an answer that I would never give it because meanwhile I changed that. So the first time that we do it we are on policy the same. The other time that we do it we say we are off policy because the answer doesn't really belong to the policy and it's a little bit sort of tricky but that's what PPO is really good at and that's why we're in the mean and we're thinking of the answer to prevent the answer from going wacky. And also as we kind of do this thing here we also have an unregularized system and now we can move into the slide. I think I'm losing that is the basis but let's we have a new agolarized system when we say that the current policy the one in real answer doesn't really have to move up from the old policy. What is new and anything all the here? Let's take this example where policy produces an answer I please that in time immediately. So this is the policy that produced that answer. This is sort of the policy that is coming into the training there but only this step of training there. Remember the policy that starts the training there we call it SFT and we call it WIFR. Let's never change yourself. At each time step I know this is very complex but for people who understand this all the techniques are actually much more simple. Basically all the techniques just take your way from here they don't know anything. They just take away which is quite natural. So policy comes into the step produces an answer and then we start to back propagate multiple times and this policy is changing but I don't want this to change too much because then we have got stronger forgetting and we do all the rest of it. So we still want this policy that is getting back propagate to be similar to the one at the beginning of the step. We call that all policy and the one the one is sort of getting get that overgator is the original policy is the new policy. And that's why we have this that we are going to look look how the fancy they are. First they say that like I'm going to make the meaning change available to me between what the model is so that the structure can sort of find this way. This is not necessarily a well-orized by sort of saying like who's contributing more to the to the structure. So I said the new policy of the whole policy. You can kind of have a bit of tragedy here to understand what we say exactly what we just need to look how careful we are. I said the new policy but also if this is to be we don't go back here we just sort of take the different between these two my comments to the changer and we just allow them to change by a delta on towards your mic. So I'm trying to say here that people are very long because we are very conservative. We say take the new change you can find it and if the change is to be well just take something there and just clip it up to 95% just a lot of 5% move your head. And then you can see here I'm going to make a mix up is all the mix up is between A and B. So look how conservative we are. And if that changes too much of the lobby and where this ex-know that sort of defines what A and B is, even ex-know, which is not able to be there and that takes that. Once we could be sort of lost we just sort of take the negative of this and we call it our policy loss. And then the value loss is just fair to be square there. And we are in an entropy term where we sort of again try to regularize and get the model to move around because look how much of a shadow we are putting that this kind of as a mover. And now here we have another regularizing the same option you want the entropy to be higher and we sort of still want like different answers. And if you look at that part we kind of guess what we are going to say, sort of try to make the model say more stuff outside of the institution. And this is the EXC which generally nobody sees it. I'm sure there is a particular interpreter but this is the main one. Policy, value and entropy. And as usual just put a coefficient in front of them because we want to move those things as we wish. And then we start back there to the own picture and we just cover the front picture. So we change here, whether they advantage and front together in a clip loss. This I'm going to show, I imagine this entropy sort of stuff and I'm going to show how the portraying data because the bug is the way to use this right away. And the value is also the variance reduction strategy. I now say a little stuff but I'm going to just sort of repeat this the more after you have completed. For any of the green and blue models. I think that the way we change the things in the future. Let me hear it as a whole but if you have a word model and this is... I mean I'm going to get caught all of that in here. I suppose this is just a policy and this is value. There are more and more. There are more than which we want. I need to run a question but I used to have a very... I didn't need that a lot of visible by hearing design this and I asked sure but for the way so. I mean for this strategy, for the... Yeah, because only we make our own current state. Oh, I mean on this side you can only just listen. You can just sort of take your initial whisper and I'm okay with the budget. Which will be forces the new whisper not to move too much. I can't have another... That's the rest of the value. You know I don't need it. Remember that I don't do it for five. Just because we follow something here is better. If you have got shoots and your data as your label, the need is crazy except for. You just have to basically your base of your... Your user, your function, the beta and you make sure there doesn't move too much from the base one. Confirm to the data and you have the data. You know what's good there. It's not a clue what the answer is. We have nothing to check it with. Which is a belief but we still manage to do some training. And get the model to improve. Again, what are we really doing? As a first paper suggests, we're sort of just doing some political efficiency. It's not that we are kind of putting new capabilities inside the model. Remember the model has all the capabilities that are out of reach for you. There's nothing that you can do about that. That's the research that people want. And all this reinforcement learning is just for preference. If I give you a question, you should give me an answer. When will you answer in the pre-training? I just wanted to be efficient about it before I asked you 20 times before I get the answer. Yeah. So the SQE already had a comment. I think I've been in the slide for a while. So the only thing that you have to pass is this and this and this. And then we'll go with the answer. Sorry, but I'm not expected to answer by you. I think it's a good learning exercise. But you see that they will ask you for a lot of options. They will ask you what the discounting factor that you want is. What do you want here? Do you want there? Like what is this sort of... What do you want for some? How many examples do you want to replay? What's your exit on that? And so I'm just a quote. And now you know why you have no disoption. So basically, I'm just taking the documentation to trigger options. Yeah. Yeah. Last one? Yeah, sure. I was wondering about entropy both at the E equation, the E equation, the E equation, about that. I thought that how was that? So after those which encourage exploration by the board of the policy of the index. Obviously, I'm doing that. Yeah, to be honest, I think it's a chance of you to question. Oh, it's just an exploration time. Okay. Last question, yeah. The benefit of not so much would be, you know, last little bit is all I have to work, not moving too far, with some other basis. Is that not also the express intent of all the other participants? I think that's the reason why I'm doing this. I think that's the reason why I'm doing this. The express intent of all the other participants in this work. And as such, like, could we just swap it? I was saying no, because PES is just, you're just training a subset of way, sir. But those ways is still moved so massively. Like, I think, I would say that that's an indirect way or trying not to move too far away. Where this is the most sound that mathematic away to say that, like, I'm trying to advise, sir, in a very specific way, by taking a revolving to a counter, asking the model to be a critique of itself, both of the actors that placed this game in, and as to kind of see if he's doing better than what he thought he was doing, and there will help in the long term. So it's a much more sophisticated way of doing it. And also, you see me that the world model is wider. I mean, PES is not going to need a revolving model just putting too much history there in these. There's no way to put this revolving model up. And remember, it does not last, it can still be that ambiguous. So what I'm going to do, you still have to go for buy-ins with action in one way or the other. So I would say that it's all very different things. Say it's...
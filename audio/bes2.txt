 We're going to explain this for you. How are we going to explain this for you? We're going to explain this for with this function here, which actually we have this one, but first we'll understand this one, and then we'll also understand the value function. Now, let me tell you a problem that you might have not solved it. The problem is, okay, let me ask you this, we're going to go back to the function and then we'll go back to the basic function, and then we'll say we're going to play the Ponga Atari Unite. Can you tell me, and we're going to play the Atari game, and we're going to get some scores, yeah? That's what this might be very well. Can you tell me the magnitude of those scores? Can you tell me a little bit of a sensor, like how big can the score be in this sort of like bar gains that you did with your friends, in those like Pong games and all those things? Can you give me a piece of an order of mine into that? Probably about 7000, so it's given you an eternity of mine. It depends on the game, right? It depends on the game. Some gains are 0 or 1 if you want, but they do it very well if you want. They don't give you a score, they say you want, and then we go in this one. Why not? Some gains are going to be real good, sir. Some gains are in the several thousand, sir. We have a problem. Those numbers are going to be very fluctuating, and I might not seem a big deal, I'll get a comment on that, and I know what standard of the facial needs are, I can calculate the Fiskor, I can take a lot of Mimax scaling, I can say that like, I can give you, let's do some number engineering, from the score of Pong, and I take it at the Mimaxing score, probably is 1 million. You ask me what the Mimaxing score could be, sir. 1 is 1 million, and 0 is 0. And there was something just came down, like 500,000 is 0 for 5, it's on its fourth time. But then you have another game, and remember that the target, or those seats that should have sort of a function that fits for all of them. Then you have another game, the C1, so now 1 will go back to C1, so the more the adjustment you have, that you know is it. But we know that the game actually is between losing a minute. So what I'm trying to suggest is that those scores are probably not good. Not only that, so there's came into really very, we can kind of adjust that, we even lost that, but sometimes when I hit the jackpot, in one of your questions, you answer, and the word modeler says, this is jackpot, like you have a lot of points for this, but then many other examples, you kind of want to be badly, or maybe you can tell when you're wrong. So when we do those things in a bachelor, we're going to have some scores where really high, and some scores where really low, and this is a problem. How do we call this problem? Before we go for the solution, like, we don't care what the number is. There's nothing there, there's nothing against 500,000, or 200 million, but statistically there's something that we don't like. We choose it. They're not really for redistribute. I mean, almost nothing is uniform. When you're one of those, I think the definition of success of goal is different between games. Yeah, basically, people are simple. I'm just not asking you to say that those are the high lines in that game, right? In that game, right? No, if that device is something that we want to put in system, we're going to see, like, some elements of bias, but they may be for me, there are 30 things that scores can fluctuate, and we don't like that. We don't like the high-by answer in between your project, with the sort of as you train the policy, the policy is not going to generate one at a time, it's going to generate 20 hours at a time, and then you can hold this up, so you can batch it. But then in the things that the basket might have high-by answer, and that's not going to be very funny. However, if you think the average of all this course for every answer, we are going to say that that's still unbiased, because on average, if you sort of just take all the things that you, and you divide by how many times you try to policy, that's still an unbiased estimate of that, because that they show average to the policy is kind of working towards that. But most of this is going to be super-exco for a second, as we move on to the next stage, and just to give you some of the, we're trying to move from fourth by, I mean, perhaps from another, but we kind of have to try to reduce this by-ticket in the remote, or it was me, and I'm going to tell you a way to reduce it. And it needs to be the model to try to predict how it's going to do. So the model is playing the game, and then when we do the model is playing the game, the game finishes, we get to move on. Then, this reward might be very big or very small and we're going to take this one. We're not going to think that the world has a teaser because again, it's very fluctuating, potentially. What we're going to do is ask the model itself to tell me, how do you think you did it? Just can you guess how you did it? It doesn't even know how we did it. We're just going to ask him, can you guess how you did it? Why we do this? It's because of all that time, the model will learn to tell us how the model did it. So we're not going to find a part of this core between 0 and 5 out of million, which is going to blow up between this, but because the model will guess when the core is there, the model will do 500 million as a score, but also for things like that, that he thinks of, she thinks of it thinks that it did 400 million and 95. So now the big face is only 5 million. So I'm kind of using the by-cer, by telling the model, by asking the model itself, how it did it. Not how it did it, how it did it. You get it. You get it with a one-on-one. So this is exactly what that band could be. How much better the model is doing compared to what the model was thinking. It was doing it. I know something like circle, I think it will be like this, just sounds a little bit weird, but it feels out of thin air. To which, the things are going to be straight through it, sort of like a band-like bias reduction, of being like, say, so and so forth. But let's think with me. And what we're going to say is that, we're going to create a new function, again, it's a delta sigma. Delta, thank you. We're going to create a new one for this, where a new kind of this, just KL divergence. Now we're going to still keep that, and we're going to use a new person for that to the party. And I'm going to call this delta P. And delta P is the water that we saw from the company, if you have, which is just a regularization so far. To us, what the model is thinking is doing. How the model is doing, and this is a little bit tricky here, so stay for a minute and a while. This is the barricade. What we think, remember what we passed the model here, we passed the model two things that we're going to go here, and this is called periscoping it. If every of the balance of model, we can come up with something that comes up all the time, we're sort of trying to do something, but not going to go there by, just sort of mentioning it. What do we do here? We kind of have a recursive function for not necessarily. So let me treat you very closely together. We say that delta P for that token is a RT plus, we passed P plus one. So we actually passed sort of the action that we did. So we've got the coin token, what we're doing now, and everything passed it. And the barricade model will turn out something else. Minus S e. Again, this is sort of just like two numbers that the value model will output for that specific token. And clearly we're going to have a lost function just for the value model, because the value model has to learn to predict how the policy is doing itself. We call this a critique. So now we're kind of moving to actor critique here. We're going to try to model up here. We're going to try to model up here. We're going to have a lost function for the value model. But your father is just trustable value model. It's not going to be that good. But imagine that it is not in this setting up. So we are sort of doing our job as training another token. Then we do something else here. We, let's see this here. And we do something here. We also define 80, which is, if you can imagine, that's what I'm talking about. Don't you? A tiny T. And the advantage of tiny T is the sort of like in the formula where we also have L-B-T plus L. Where L is something like sort of projection if you step in the future. So I just want to go through there and then go to the picture and I'm just going to create something we get. And then finally, what we do is we take finally our own order for that specific token and we treat it as like that. That's where I'm going to show you how to calculate it. Plus what the model thinks we do from there. Let's go through together now. In a little bit of sort of picture. And I can maybe we can go back to tragedy where it tells you we're more comfortable and soft. Let's go through it. So now we are sort of trying to work towards those three things. It does a very important thing in reinforcement value. They sharp everywhere. And they're super-protected. Remember what are we trying to solve here? What are we trying to solve here? How are the challenges for a person? But specifically, why would it do this madness? Try to ask for a reward across the whole thing. No. So do you understand how well you do it? No. It's a comparison. You know, I said, you know. Remember, because we want to use value. Oh, yeah. The reason is because this rewards can be very weak or very small. And we start to sort of our spreading that cannot be a naive spreading. And we take 500 million. And we just spread it that we do. We say the first time we get one of the million reward. The second one we get, one of the million to us, 200 million now. Until all of them somehow, somehow to see something new. The greatest is just below our head. So what we say is that at each time sector, we're going to ask the model, how do you think you're doing it? So what do you think should be the score here? And there was sort of like at the same time, we spread it this score. The fire at a million was sort of spread it through a talk answer. But at the same time, we tried to calculate it. And those two things kind of went as simultaneously. Like as we spread it, we also tried to calculate that what the model thinks should be in its score at that specific time. So we can just sort of track the two things in where the factor is. So how much the model is doing better? How much the model is having a factor compared itself, compared to the value function, the model is predicting itself first? Again, the only reason is because the dimension we want to do is that the 6th and the million, what the modeler to predict that the model is doing 500,000,000,000,000,000,000, so that they're just taking the factor 10 million, which is still lesser than 6th and million, so the greatest value not lower as much. Here it can be that the model will go up. So we're not shaping this, we're not shaping this, we're shaping this a big thing in and of itself. That's what we're going to do. Sorry, were you saying the amount of gold that it didn't say can get given depends upon how well it is to do it? It's up, yeah. And now we're going to have a little more like how does this happen? And then I promise you, once we do this, we're done. Because this is sort of like smooth sailing. But I will say that this part here, I'm going to be the trickiest part. Yeah, actually I'm lost with the C7 because that's it, we were projecting to the future because we're ready and then the 7th was also how well the model, the value model, the primitive, so what? It's a bit tricky. I will try to go through it, and leave it as a water. Right now, so hopefully whether I went out to show you a couple of holes I said, just leave me a second bit.
 many because I've never done it before. And there is not never done it before, because it is genuinely one of those things that is so easy just to throw it around and say a lot of built up to reinforce the learning across the polyseptimization, which is one of the sort of big outreach of set. There was a development in 2017. We have a theme to see if we can do it for the clinical. But nevertheless, let's give it a little bit of a problem. With a bit of math and a little bit of understanding of why you do this, like what problem are we really trying to solve that? Before we jump there, Marla, this is what we're trying to do. Sure, we want to get a mother's son, who doesn't. And maybe to give you a bit of a sense of easier, as I said yesterday, why we do all this math, is because ideally, we would like to get 20 million samples of supervised fine clinical data sector. 50 million, 100 million units, must have a lot of data so that we do retraining there, there would be supervised fine clinical and we have business to do that. We cannot do that, so to come up with all sorts of tricks, to approximate it or to allow the mother to do it by itself and find some proxies, some approximators where we can kind of do what that massive as a people have done. I will always simply do that. Yeah? OK, so this is the big problem we're trying to solve. The way we're trying to solve it is, and this is the whole sort of, what we've always been learning background with our hair is, is we're trying to, we don't have, there is a field of study, which I'm not necessarily sure is the type of shellur, and surely is a little bit around it inside or outside, called Winforson learning, and both of our greatest computer science back from my 50s, but the main idea is that there may be, if I'm an environmental actor, we can have an agent here, the sort of production of them, the action does produce a new stator or an ex-sister, and maybe it would produce some of water occasionally, ideally all lesser, but we are a bit of a drive. You not always get the one water for your efforts at every single moment, so you kind of have to be long thinking. So we call them Sparser words, and those can be back into the agent, and the agent can learn, promote this with water and state, and then it can give you another action. And if you do this love enough, hopefully, the dream is that we're gonna get AGI. So we're just letting the computer in the water and we learn what to do. And we already know this, our friend Transkola, which is really good, our storing maximum amounts of data, and that part here is an efficient knowledge extraction. That's not a state, because although, if you probably know the big literature, like when something called AlphaGo, like quite few like New Zero, these sort of light-winkers, and now we can achieve superhuman performance, which really means that they start, and they do back to the new one. So maybe if we can crack this with our lens, so we're gonna get an LLM, that somehow will be here better than the original data anyway. We'll kind of learn to be certain, if we just sort of spit out an extoculture, and then we'll learn to think here, and we'll do better than the humans can do. This is like a bit of like a dreamer, but there was this paper coming up like, at the end of May, or in May. I don't want to make it too much, but they kind of make a point that that, so far, when Cosmic Learning has not been something that can give superhuman performance, but a backstair can extract the knowledge of their LLM much faster. So if you have a base model, you have to give it a few shots, and learn it's a few examples, and then ask to answer your question. Or you might even ask the base model, many, many, many times, the same question, and you kind of open this temperature, and you always wanna get to an answer, what they did, what they noticed is that, if you just try to sample one cell from a Cosmic Learning model, a reasoning model, and from just a base model, there are Cosmic Learning model is way more efficient than giving your correct answer. So if you just try one cell, for example, I don't even know what those are, so they can be accepted. If you just ask it one cell, the base model, only 20% of the time, will give you something that is useful. So which means that you have to sample five times to get your answer. While the Cosmic Learning model, there's going through reasoning steps, so you'll go to the right step. 40% of the time is immediately the result. So get double the efficiency. So like, you don't have to ask it many, many times to get the answer. And it's kind of, they did a lot of like, take a step further, and they noticed that maybe what the Cosmic Learning is doing with reasoning is we will just sort of increase some coefficient. Yeah? So basically all we have to say is that we're not giving back to the base model. We hope we boot up, but you come to me with the Cosmic Learning model, and I will take the base model, that you train your model up, and I will do better than you. No matter how much the Cosmic Learning is doing, I will beat you. If I ask the base model, 128 times the same question, and I kind of have to wait through that. And I will say that I will do much better than you, which you are just going to ask your Infosmic Learning model, because actually you went for a bit of this catastrophic forget to clearly not expect it, but you're kind of tapping out the Infosmic Learning model because you are lying to give you what you want. That is actually a musical performance. So it seems that we can learn better than the base model. I like the big after-ex here, but I'm sure people have had my sequencer, because this is the best shot that we have. Or just having a better manner, quality in an agent, putting in everybody's love to her, and come back in three months later, and we hope that this will have become so much better, but that is not necessarily the case. I'm going to make questions on the other, I'm just sort of keen, if you understood the base idea, which is, we start from a base model, you do an Infosmic Learning unit, you come back to me and be like my model is better, and take the base model, I'm sure to fool you, that's not the case. So what is the exact same thing as the number of times you sample from the first model? Oh, the number of questions. Same question, how many times you asked it? So you allow the model to sort of try also to waste of the next token thinking and then something else. It wasn't the wire. Is I'm in doubt? I don't remember, I think it's like, that they are at least six. Okay. So do you increase temperature, or you just change the seat? I think here is a bit amateur. Okay, so this is kind of a big view, like why we do it? We're kind of still doing because, we want to know the destruction and the sample efficiency. And we don't want to ask the model 20 times to get faster. You don't want some times to just go away because it's taking too long. Okay, that's one of the most. Now, let's go to our guide. So this is what you view, Dousa. And we're gonna go through it, step by step, besides this part here, which I'm not going to show what they're doing. Yeah, I think I can guess it, but I didn't have time to go through the picture. I was busy aligning my mind to talk. So I already did it. Okay, so we're gonna jump between this lighter. This is light, which has a sense of depth, but just in an enemy speed light. So, the way we're just gonna go through that one, and even here we're gonna go through them, but besides number 13, which I'm not really sure what they're doing. And also number 12 protection. But those are just regularizers, so I think you know, a bit of a sense of what they're doing. And then I think this is the most important part, something that people call like their fundager, which has way too many nice properties, but it also means a pain to walk with them. Before we jump there, something that is not here and here is the world water. You were faster to also train a world water, so let me start from here. What do we do? We take our supervised function modeler, we start from here at the bottom, and we call it a policy. That's the first step, super easy. Or with me, which really means that this policy is still able to do something, but we finish the money. And we don't have anymore life executors to pay new months for. So now we are left on our order, because the model is kind of okay, but it's in fact. Then what do we have? We have in theory, one, two, and three. We have three models. Oh, but they're all the same model, in one way or the other, because they do different tasks. So let's focus on the first one, which is this one. I'm not going to be the actor, I'm not going to be able to. But the first thing is that this is our SFT model that we just call policy. We take also our SFT modeler, and we do some training, so that the SFT model now is called the reward modeler. And the only thing that is good after is given to answer itself with the last wish on Inspector. Now, talking which one is better, but we're given us a score of how to do the design. The way we achieve this reward model training there is by using this law structure. So we take our SFT, and we pass to the SFT two answer itself, which I'm not going to be in law, clearly because the SFT model is okay. So we take a question, we pass it to the SFT. The SFT gives us two answers. Just two answers, so we present our reward modeler. The reward modeler, what is the reward model, by the way? As I said, is the SFT model, but the only thing that we change is that in the last talker, we actually go to the regression tasker. So we don't project and move back to the vocabulary, but we just take the hidden stator, and we project it to a number. Because clearly, our SFT model is a language model, we just predict the next talker, but the reward model is not that predict the next talker, is that predicts on a number. How do we do it? We change it. So to take the hidden stator, and just to check this one. So that is called the SFT, is that predicts the instruction that our giving post, the order of post, and then in the summary, and after asking to guess the number. Yeah, or by the last talker of the summary, is the one where we project, I mean, we project it all of them, we kind of ignore it. Indeed, actually, we can't ignore all of them, and we just take the last one of them, and then we sort of project the last one from the hidden stator, so just to make it clear, in the example, have another value, which is better, but what goes inside the reward model is the pointer, in this case, like the big cost. The ballets talk about question answer, which is a bit easier. We put the prompt, which is the question, then we put the answer, both of them together. And then we change the reward model, just the last layer, literally, the last projection, calling it the header, so the model header, and we just sort of project it to a number. Only of the last talker, because clearly, we do cause a lot of tension, so the last talker has seen all the previous ones, is that the top product with all the queries and keys has answered for them. Again, we don't know what those numbers are, so we can't really look at them, and be like, four is good enough for this, so 2.5 is too low, sorry, support. We can use in a sort of a approximate loss function, the self-care sense, that you know what, we just want the difference of them to be high. We don't really care about an absolute number, at this stage, because we don't know it anyway, but we can just say that, like, the numbers have to be far apart. And those are kind of pretty little bit, but this is probably what you've seen. We put the same question, and then the one, the more the both, we sort of have meaning answer, and the losing answer, and you just hope that the difference is bigger. Why do you maximize the log of the differences, instead of just an absolute difference? I think you need to increase, I mean, don't just say just numerical, but the C point will go from zero to one number. Why the log, yeah, we can't even just like no linear, because we're still like zero is much infinity, and it goes up to one. Okay. Well, I think it may not be my necessarily the case, but I can even, I usually just like the mic, as the infinity. Okay. Another choice is the case stuff. Okay. Yeah? Oh, I'm sorry, I have another question. So in the last, just to clarify, so in the last, I think I'm confused between, I have changed the reward in the last, like the last bit, the best, the last bit. This is in the last one, should we have this happens? Is that okay? Yeah, yeah. And clearly, this is going to get trained all of these stuff, in your case, we just trained a lot of them itself, because you do attention, and affection, everything talks to everything else. So the kind of the greatest will just flow through the entire model, and it's not that you just go lean on it down, as you go. We found some really good information. So for this model, would you take the pre-trained model that we've found to him before, or would you just take the structure and train it again? I mean, people make sense to take the SST. Yeah, that's what we did, and it seems all right, but we saw some information that's about data. They don't write data. They take the data. I mean, if he is looking more clearly, trial, sort of things, then you see other changes that, they give you pure data, and can you give them a data, and then they give you back a data. So if you open it out, it changes to the way they deliver the data. So this is clear, right, though? We're just training on the model. Now, let's go back here. We have, so now we have done, you know, we have the SST model, yeah? We have our own model, we have a very small model, which is the SST where we change the name, at least a few years ago. And they give you another model, a value model. Does anyone know, and this is a kind of a genetic analysis that we're gonna work towards, is that what is the value model? This is a bit weird, like we don't expect it. I mean, I just, you know, probably you're not seeing any of the handplats, but you're not seeing a value model before. What is a value model? Is it the adjusted reward model that I've added in the TPO? No, it's a bit of a low-dissquestion, so it doesn't expect, like, the answer will take by the mean of time, right? I just wanted to put you out there to also have a value model. How do you think we would put strength by the model? There's no mean there's a lot of ways. No, we take that step here. Even better action, we do another trick here, where, and this is what even open-air, I change the little bit there, the value model is, here they kind of do this and this, but the value model, most of the time, is the policy model itself. With the only caveat, that the, we change it, and we add another header on top of it, the next to the big third are scalar. And we're going to go now to the policy, and I'm kind of going to mention it a little bit again. What is the value? Yeah, there's a good point there. What is the value? I'm going to talk about the value and I'm going to talk about the advantage, which is this nice general advantage estimation, or we're going to go through it, for three. So, what they don't do now is that the value model that is basically the policy model, but with another header on top of it. So, let's go into this one here, and I'm going to spend quite a bit of time here, and I'm going to give the first two formula, what's the initial definition for a fine? It's something. Okay, so, what is the problem that we're trying to solve again? We take a question, and we ask the policy to give us an answer. Except that that's what we want. Then this will do it out all the questions. This is your normal letter, that you read your down to four. As we know, this will sort of produce a bar of probability distribution, or what should be the next token of collector. In this case, this case is not like super line, but I imagine if you have the staff token of it, and then it's a log, then there's a log, and yes, there's mine, and so on and so forth. So, this should be a little bit of a disaligner, but this is just some of our time to make it. Now, we need, and this is for the pointer, we need to compare those things to something, but we don't have that something that, we're kind of used, you got it easy so far, because you put it in a log, then we have the source of ground shooter that maybe should have been something else, or we can just look at it, and we say, okay, what is the difference between this probability distribution of the model, and the truth of the distribution of the data sector? And we can kind of calculate the divergence, and we can call it a concentrationary answer. Yeah? Here, we don't have, but we still have to do that for potential. So, what can we do instead? Please, please, be able to treat your question. But we have to come up with a number, because before we do a concentrationary, we need to do something that we had, and then we come up with a number, then we do this with something that we got, and we come up with a number, and then we try to take that temperature, and we do that for the time. Now, we don't have that, so we're going to come up with something else. And this is the formula between you. This is just parts of the world we're going to use, and now we're going to talk a little bit about that. So, what do we do? We use that we're a world model. Once we have this data that gets out of the generated data, the world model is going to give us a score, if it's the world model. And now, we kind of have to do something that we call the world model shaping, where that score is for all data. But we don't want that, remember that we need kind of a loss, surely we could just back away here, right? Like, the world is going to flip the people to you in a sense, that's a little bit. I do this, the policy will give me this answer, then I pass to the world, and the world will tell me, five. How do I back up again? I have a five in my hand, and I have a bunch of sort of like all of these, what do I do? Surely as a reminder, it's obvious that somehow I should kind of take this five, and kind of spread it somehow across each token. How do we do it? It's massively in a free searcher. GFB, all the people that come up with all sorts of, well, like algorithms to do that. And this is called a word shape figure, which comes out everywhere. There's a problem, though, whether that, actually that kind of solution, we can take this five, and now we have to sort of put it around there. Let's see how we do it, and then we can sort of, here we're going to do some beautiful joy here, but again, in the managed show, but let's work towards it together. So, I'm gonna change a little bit in the main picture, but not really. Like here, I say that, and here, those are questions that I can want to put here, and double-check the charging case. So, let's go in the master. Okay, so notice what we are. We're kind of going through it now. We, the first thing that we do is, we call ST, our sort of, how can I say it? How can I say it? We start with things with different names. Like any token, so T here stands for time, no potentially, but our case also stands for token. Because sort of, it is easy way, if the way you're doing it, it's a nice path of isema. So we say that, like, you know what, we invent and you all, or then we call it ST, and we say that this thing is anything that came before this token. Not to this token, but that's just everything that came before it. And again, you can see that this is a question, and then we take every token of the answer, so up to T minus one. So let me kind of just go back here, no, in the picture. So, what is ST here? ST is everything that came before. Easy. So that's ST. Let's get started. What is the answer? The prompt? The prompt. Then you have another thing, we say that an epsilon T, which is actual like output, in that sensor, it's not part of the state, because we just said that thing before, but we call it an action. Because that's what the modul is doing, and that's specific to our amount. We're not doing anything special, I'm giving them some different answer, but you have many number of those things, all with me. Now, the final new figure, and we call it a one-water, up time P, and here we have business. Remember the problem, we need to give to each of those tokens, a scalar, so that we know how good it did that. But boy, that's gonna be a big challenge, and you can rinse it. So what do we do? At each time P, we do those two operations, so one and two, let me go back to it. So everything is covered in token data, what's the element of the model? And what we do is, we do this one, we use that, let's go for it. The first thing that we do is, we calculate that the reward of hello is basically just a reporter. The reward is calculated that there is a difference between, that from a billy, the likelihood of hello, which is basically, on a second this year, I just forgot. Okay. As I say, this is gonna be like, I'm not getting more views together, but okay. It's a celebrity. That's it. You might be just gonna be taking this, and I could be touching this right. Excuse me, I'm sorry. Excuse me. It's not a big deal. It's not a big deal. It's not a big deal. I'm not gonna, if we try to make these diagrams with my jam, I'm not gonna, no. No, I'm not gonna. I mean, I'm gonna take my own models to do those things. I mean, I'm for example, say one in one place. But no, I mean, this is like, I mean, I have to do it in my hand, like, you can do that there. Or to close the boxes. Okay. Okay. Hey, yeah, we have some, we have time, okay. Okay. Okay. Where are we here? Okay. Okay. Okay. Clearly, we'll be correct. Okay. So I want to show you what I'm doing here. At least I just, we were able to listen for jumping this in this diagram. So what do we do? For each token, we know that the policy predicts that there one token. Why could the policy give us for the distribution? And then it's up to you, which one you want to take? Most of the time, we take something that is high probability. Like, we do something a little bit with temperature. So we can't really go into the biter, but you can easily just take care what the model suggests. And the most of you do something, which is the most probable thing the model suggests it. However, if you put the same sort of, if you put the same answer that the policy gave to the Wi-Fi model, remember this at the beginning, they're all the same. So it's quite different to be basically zero. But all the time, the policy will want to change, will because it will get some rewards. So the way, but we don't want it to change too much. So we still keep the Wi-Fi smaller than one of them. The Wi-Fi smaller is the initial SFT that we have way back at the beginning of the training. And that stays frozen forever. We just use it because every time the policy produces an output, we send each of those tokens outputter to the Wi-Fi smaller than one. And we tell Wi-Fi smaller, what do you think of this outputter? Is this something that you would say? And if the Wi-Fi smaller is decided for a bit of me, say a name is very low, because now the Wi-Fi smaller will have a different probability. Well, we want to take that into account. We want to take the fact that we're moving away from the SFT model, from the Wi-Fi smaller model. And that's something that we kind of want to, we see that this is a bit of like a pool and a pool. We kind of want to move away from the Wi-Fi smaller. We do want that, but since that we don't want to move too much. So we're going to constantly calculate, we do these two kinds. And we also do the Wi-Fi smaller model that we're going to talk about in a second. So we're going to use positive GPU. So far, so we know that's a good question. We're learning how to back propagate, which means how we're training, but on the other hand, you're telling us that we have reference in policy, which are not the same. So this assumes there was some training already. Yeah. At the beginning, the first step, I thought there were going to be a same one, but then there are many people who are changing over the sort of key keeping. So policy is the GPU model, when it means there. Yep. The wrap is boarded in the tank. And I thought it was the right way. Yeah. There's more. Exactly. The fine tune. Is that I? Yeah. Yeah. Yeah. Yeah. Yeah. And just the distance. Should I run this way? Yeah. OK. So where we're here? We're here. So all I'm telling you now, we're already moving out because this graph is going to be very clear. What I'm telling you is that I think I'm using query. I could be here to get in the order. It's just a policy. We pass it to the policy. The policy will create the, you see? Clearly, we're just sort of outputable. That's what decoders do very well when the topic was true. But we're not going to be divided, because we're not going to be interested in training and access the question. I'm not trying to order to sound like questions, sir. We just trained on the answer. And here, I remember the name before. We kind of started to call things dictionary. We said, no, I'm going to call F.E. I'm going to call stator. Stator T, just end it here. And I'm going to call action T, just a sort of specific token. Clearly, this is going to move, but this is just not my culture. And this is sort of just a change of, a change of mindset. Then we send the same, all of these, so we send this again to the SFT. You see? Because we want to see how far, what does the SFT think about this, this concept? At the same time, we send all of these to the bottom of the, which we're not talking about that. And we also send all of these, to the remote model. So now, somehow, when you par with, we call three models, with our answer. And we have to do these, because remember, we don't have a source of truth. So now it's about to read the engine, and to be like, OK, how could I face this answer? Let's put all the triggers, sir, and see what we can do. And I'm going to talk a little bit why we have some of those things, sir. But I just appreciate it, and the mechanics is correct, is that how poor? Is that how poor? I do not know this is one. Oh, yeah, it is a poor. That ain't number, yeah. One, two, four. But it's always the second one, that we sort of just split into four. But yeah, the four, that's the, so, OK? And I'm going to just take the next field. Gradually changing from the width of the reference, for just main aim, but that's like, we're changing our way to express that. You mean what do we want from, like, what is this for? Yeah. And because I can just think, like, no, we don't want to move to far away, because we might easily collapse, so we remember what we say in class week, if you just pass through the model, my name is VESA, and you backpropagate three times, so the model which is this, we forget everything. Because you're just moving all the way, sir. And if VESA is, or my name is VESA, is the only data-centric piece in that world, the model which is going to backpropagate in that direction. So when you ask if, what is a car, it's just going to fly. So we don't want that. And here we have one example of the time. Can you imagine how easy people that be for the model to completely go lucky? So K-L, we don't want that. K-L, when we're realization is something that is used everywhere. Yeah? But we're going to talk a little bit more. OK, this is going to take quite a bit of time, I'm going to say. I hope for a moment we'll be able to hear that. That's a good fun. So we're generating the seconds. You put it into the positive. Yeah. You put it into the positive, because your Terry Bradshaw was which he used to train the model. And it was imperative that's why he used to be up there. No. No. Like that's a big, that's a big goal. Is this a different, OK? That's sort of a function. You just used that to create a world model. And then we started to wave. And now we just care about that. We say we will alter the policy, which we need to ask is a question. The policy will give us an answer. And we just use the world model to tell us how good is the answer. But we're not going to be able to take it. But we're going to hold it up. But that's all we can get. It's the same question as before. And still don't get it how, if you need two different things in order to progress your back propagation, how do you even start that? Like is it random? We just walk into a random direction. You mean the policy and the FD need to be different for you to get that function you showed us before? Yeah, the first step ever, they're going to be the same. Yeah, but then how do you even change? Because there is more to it. There's a lot of them when both functions get a little bit more complex. But this is the decision. But you know why? They also don't. We don't like it. What's the problem? Sorry. Do they also not have a small head or something that differentiates them or are they that share their potential? The first time they should have. And then part of the assumption, the part of their world model, the member of the trying to do better, we produce this, but we have no clue. We can come up with something like this. So we'll come up with some numbers. So we can start that propagation. This is the first step. We say, OK, this number, let's do this. This number, the first thing, the first number we're going to produce, because that candidate for this is simply the difference between this and this. This is the difference between this and this. Again, I'm just telling you the mechanics. I'm going to say the first candidate for this is the only chance for difference between those two. Which I tell you just a regularizer. But they came up with it, and you just have to go. Because there are quite a lot of things in this part. So the fifth one will be changed. And after this, they will be changing. No necessarily. No. I mean, everything will change at the end, because they are different goals. But when we're going that way, I will just stay here, which is like, we've got the calculator of this sort of score for the specific token. We're not finished yet. This is just because we're going to have six of them, probably. And then you all have to get everything out of the way. So to give us more number. So let's go with it. Just let me know if you understand the mechanics. No. There are just six people three records. I'm not thinking. This one? Yeah. No, I'm not talking about it. Just kidding. No. I'm just talking about it. Just kidding. I'm just talking here. Yeah? Let me just give you one more time, and then we'll go back. Look what we'll do, then. Again, it is not the story. So don't ask me a question like, this is nothing else. Over here, obviously, from the standard. So we ask the policy model, which is just SFT. We ask it a question. And then the model is how to aggressively weigh, produces an answer. As it produces this answer and it's finished, we send it, forget the wrong word for a second, we keep structure of the probability, the output probability, of each token. Then we take the same answer, and we put it back to the reference modeler, and we see what output probability to the reference modeler gives to each order. The first time that we're going to get to come, but as you will see, the policy will change, but the reference model will not change. But at the same time, the answers will do change. So the reference model is there, okay, well, you know what this answer? I'm going to give an endless probability. To each of the tokens, they will give them a big, quite a probability. Again, most specifically, can be a different life of the product. So, a likelihood is the thing that you sort of can write and look on that epsilon-hust is of a density structure, right? By feeling like a discrete body there, and to be the same probability, and that to be a... So the likelihood of this stuff, and it's basically just a value, we kind of think a lot of it, and we sort of just subtract it for the reference. So this is the first part, okay? For each token, we just have one in this. We also give it a coefficient, because it's always master. We said, how much do we care? The two things have to be similar. If the coefficient is super big, we're going to buy the moment and we need to keep the rest of the work, where the policy is super close to each other. If this coefficient is zero, we just say we don't care. They can go anywhere they want, as far as they want. And clearly, this is your first hyperparameter of the menu we're going to see. What is this little type of thing? We said that for each token, this is the first combinational reward for this specific token. Now we say that the last token, as I said, you can see what it is. The probability told me that only the last token, gives an extra reward score that came from the reward model. You remember we passed the same answer to the reward model and we brought back here a score. And we put this score here. But it was happening now. We have a problem. And we have an extra next telephone, or all of them here will go like a candidate for reward. But that candidate for reward, this is just how much we stock an interest in the reference, compared to the policy. Sure, even I was based based in just regularization. We're not really into the device, we're going to do anything besides take close to that safety, which is a layman sort of optimizer. But we did something now. We gave the last one, we gave the reward, the complete reward. So this value, like an investor, is like five, whatever it is. That's what the reward model decided. Now the next step is going to be how we get this fibre to splinter. Remember, we're always trying to do this kind of thing, and I already told you before that we're going to do something called the reward shipping. And now I'm going to take it in the reward, and then the reward shape will be the complete advantage. I'm going to take a lot of advantage, and then we're just going to hold the box of reinforcement learning there. But let's see how far we get. So now just to clarify, here we got all those numbers. As I said, in the previous, the last one, we were going to put our task into a square. Why do you only get a reward for the last one? Because that's how we decided to do this. As you can imagine, there's a question left, if you have other things, you can seek the decided to split the reward by far rather. The decision to make for far rather, I'm going to give out the reward. And they think some sort of interesting things are to do with it, because we have to work with this a far rather, where does it end? Did anyone ever do one token, one reward? Would that even make sense? Well, remember that each token, we get a reward. It's just how, whether we reward from the reward model, we show up, or how it's spread there. Maybe it's sick when you give this a reward, or whatever reason. And then this reward, that has to kind of be spread back. But if everything had a reward, why do you even use to spread it? Well, I mean, how do you want to reward more than an additional token? Okay, so that's the limitation. But how do you want to do it? Another way to say it, maybe the reward is for the whole answer, and not for anything at all. Okay. Okay. So that's simple, you can see that it's function margin. Yeah, this is true only when it's more key, when the token that we are considering is big team, which is the last token. So we say, this is always zero. And unless we are the last token, which case is one, which means we take the reward that came from the reward model. So it's a Chronochemacadab, indexes the same. Was the name register? Like Chronochemacadab, is it the name? Yeah. It's one of two indices on the same, or one of us. Exactly. So the last token is equal to t. It's one, otherwise it's zero, which does exactly what we're talking about. I was asking for names that I would like to visit, maybe I'll go get a one. Okay. Are you with me, Sapa? We'll give that. Just a sort of minor pointer, this little approximator, this is not a true key in divergence, but it's an approximation of a key in divergence. Mainly because the two are sort of different between two probability distribution, you need actually the flu, but we need to distribute that. And then you can kind of do that. If you're just two points, that's sort of an outcast for one. Just sort of a minor, if you have one in one, then you can. Okay. Let's talk. Oh, yeah, this is right. The number of names you do. Any class, or so. So this is going to be interesting. I was thinking about how to do it. And if you saw me doing this, I was going to tell my daughter, how many concepts do you know? Let's take this key out. Come on, let's take the jump between. Let's see.